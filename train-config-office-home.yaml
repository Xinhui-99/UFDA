data:
  dataset:
    name: officehome # choices are ['office', 'officehome', 'VisDA+ImageCLEF-DA']
    root_path: /home/xinhui/data/OfficeHome # /path/to/dataset/root
    domains: ['Art', 'Clipart', 'Product', 'RealWorld']
    target: 'Art'

    n_share1: 4 # number of the common classes between the No.1 source domain and the target domain
    n_source_private1: 2 # number of classes private to the No.1 source domain
    n_share2: 4 # number of the common classes between the No.2 source domain and the target domain
    n_source_private2: 2 # number of classes private to the No.2 source domain
    n_share3: 4 # number of the common classes between the No.3 source domain and the target domain
    n_source_private3: 2 # number of classes private to the No.3 source domain
    n_private_common: 1 # number of common classes between the private label sets of No.1 source domain and the No.2 source domain
    n_share_common: 10 # number of common classes between the whole source domain and the target domain
    n_total: 65 # number of classes in total

  dataloader:
    class_balance: true # if class_balance when loading datasets
    data_workers: 0 # how many workers to use for train dataloaders
    batch_size: 32 # batch_size for source domain and target domain respectively

model:
  base_model: resnet50 # backbone feature extractor
  source1_model: resnet50 # backbone feature extractor
  source2_model: resnet50 # backbone feature extractor
  source3_model: resnet50 # backbone feature extractor
  pretrained_model:  ./Pre-trained/resnet50-19c8e357.pth # /path/to/pretrained/model
  pretrained: True

train:
  min_step: 10000 # minimum steps to run. run epochs until it exceeds the minStep
  lr: 0.01 # learning rate for new layers. learning rate for finetune is 1/10 of lr
  weight_decay: 0.0005 # weight_decay for SGD optimizer
  momentum: 0.9  # momentum for SGD optimizer
  continue_training: False  # continue to train on resume files: True / False
  continue_step: 0  # the step continue to train
  cut: .0  # cut threshold for normalizing weights

# The configuration for our decentralized unsupervised multi-source domain adaptation
UFDAConfig:
  # As stated in paper, we gradually increase confidence_gate from low to high
  confidence_gate_begin: 0.9
  confidence_gate_end: 0.95
  # Controlling whether to use the batchnorm_mmd
  batchnorm_mmd: True
  # the communication rounds in decentralized training, can be set into [0.2, 0.5 ,1, N]
  communication_rounds: 1
  # the malicious domain with poisoning attack

TrainingConfig:
  # The total data numbers we use in each epoch
  epoch_samples: 15000
  total_epochs: 50
  # We decay learning rate from begin value to end value with cosine annealing schedule
  learning_rate_begin: 0.01
  learning_rate: 0.001
  learning_end: 0.000001
  gmm: 0
  p_threshold: 0.9 #the parameter in gmm

test:
  test_interval: 100 # interval of two continuous test phase
  test_only: False # test a given model and exit
  resume_file: ./log/WD-A/best.pkl # model to test or continue to train
  w_0: 0.5 # the threshold for separating unknown-class samples from known-class samples

misc:
  gpus: 2  # how many GPUs to be used, 0 indicates CPU only, needed GPU memory < 6 GB
  gpu_id: "0,1"  # which GPU you want to use
  gpu_id_list: [0,1] # [0,...,gpus-1]

log:
  root_dir: log # the log directory (log directory will be {root_dir}/{method}/time/)
  log_interval: 10 # steps to log scalars